and now what we will get into in the next chapter is we will start talking about artificial intelligence and this is titled as from the spring to the winter of ai so i am going to talk about when was this boom in ai started or when is that people started thinking and talking about ai seriously and what eventually happened to the initial boom and so on right so let us start with one thousand nine hundred and fortythree whereas as i was saying that there was a lot of interest in understanding how does the human brain work and then come up with a computer computational or mathematical model of that right so mcculloch and pitts one of them was a neuroscientist and the other one was a logician right so no computer scientist or anything at that point of time and they came up with this extremely simplified model that just as a brain takes the input from lot of factors right so now suppose you want to decide whether you want to go out for a movie or not so you would probably think about do you really have any exams coming up that could be your factor xone you could think about is the weather good to go out is it training would it be difficult to go out at this point would there be a lot of traffic is it a very popular movie and hence tickets may not be available and so on right so a brain kind of presses all this information you might also look at things like the reviews of the movie or the imdb rating of the movie and so on and based on all these complex inputs it applies some function and then takes a decision yes or no that ok i want to probably go for a movie so this is an overly simplified model of how the brain works is and what this model says is that you take inputs from various sources and based on that you come up with a binary decision right so this is what they proposed in one thousand nine hundred and fortythree so now we have come to an artificial neuron so this is not a biological neuron this is how you would implement it as a machine right so that was in one thousand nine hundred and fortythree then along and then this kind of led to a lot of boom in our interest in artificial intelligence and so on and i guess around one thousand nine hundred and fiftysix in a conference the term artificial intelligence was formally coined and within a one or two years from there frank rosenwald came up with this perceptron model of doing computations or perceptron model of what an artificial neuron could be and we will talk about this in detail later on the course and not tell you what these things are as of now just think of the a new model was proposed and this is what he had to say about this model right so he said that the perceptron may eventually be able to learn make decisions and translate languages we find anything odd about this statement yes so learn and make decisions make sense but why translate languages why is so specific why is such a specific interest in languages right so that you have to connect back to history right so this is also the period of the cold war and there was always always a lot of interest a lot of research in translation was actually fueled by the world war and even said happened after that where these countries which were at loggerheads with each other they wanted to understand what the other countries doing but they did not speak each others language that is why there was a lot of interest from sp on edge point of view or from spying and so on to be able to translate languages and hence that specific required and a lot of this research would have been funded from agencies which are interested in these things right and the defense or war or something ok so and this work was largely done for the navy and this is an this is an extract from the article written in new york times way back in one thousand nine hundred and fiftyseven or fiftyeight where it was mentioned that the embryo of an this perceptron is an embryo of an electronic computer that the navy expects will be able to walk talk see write reproduce itself and be conscious of its existence so i am not quoting something from two thousand and seventeen or eighteen this is way back in one thousand nine hundred and fiftyseven and fiftyeight right and that is why i like the history part of it so recently there is a lot of boom or lot of hype around ai that ai will take over a lot of things it will take over jobs it might eventually we might be colonized by ai agents and so on so i just want to emphasize that i do not know whether that will happen or not but this is not something new we have been talking about the promise of ai as far back since one thousand nine hundred and fiftyseven one thousand nine hundred and fiftyeight right this is not something new that people are talking about now it is always been there and to what extent this promise will be fulfilled is yet to be seen and of course as compared to one thousand nine hundred and fiftyseven fiftyeight we have made a lot of progress in other fields which have enabled ai to be much more successful than it was earlier for example we have much better compute power now we have lots of data now things to the internet and other things that you can actually crawl tons and tons of data and then try to learn something from a data or try to make the machine learn something from data so we have made a lot of progress in other aspects where which ai is now at a position where it can really make a difference but this wanted to say that these are not things which have not been said in the past it has always been that ai has always been considered to be very promising and perhaps a bit hyped also right so that is about one thousand nine hundred and fiftyseven fiftyeight then now what we talk about is for the past eight to ten years at least when we talk about ai we are talking about deep learning and that is what this course is about largely about deep learning i am not saying that other and what deep learning is largely about if i want to tell you in a very layman nutshell term is it is about a large number of artificial neurons connected to each other in layers and functioning towards achieving certain goal so this is like a schematic of what a deep neural network or a feed forward neural network would look like so this is again not something new which has come up in the last eight to ten years although people have started discussing it a lot in the last eight to ten years look at it way back in one thousand nine hundred and sixtyfive sixtyeight suppose something which looked very much like a modern deep neural network or a modern feed forward neural network and in many circles he is considered to be one of the founding fathers of modern deep learning right so that is about that sixtyeight right from one thousand nine hundred and fortythree to one thousand nine hundred and sixtyeight it was mainly about the spring time for ai and what i mean by that that every one was showing interest in that the government was funding a lot of research in ai and people really thought that ai could deliver a lot of things on a lot of fronts for various applications healthcare defense and so on and then around one thousand nine hundred and sixtynine an interesting paper came out by these two gentlemen minsky and papard which essentially outlined some limitations of the perceptron model right and we will talk about these limitations later on in the course in the second or third lecture but for now i will not get into details of that but what it said that it is possible that a perceptron cannot handle some very simple functions also so you are trying to make the perceptron learn some very complex functions because the way we decide how to watch a movie is a very complex function of the inputs that we considered but even a simple function like xr is something which a perceptron cannot be used to model that is what this paper essentially showed and this led to severe criticism for ai and then people started losing interest in ai and lot of government funding actually subsided after one thousand nine hundred and sixtynine all the way to one thousand nine hundred and eightysix actually this was the ai winter of connectionism so there was very little interest in connection is ai so there are two types of ai one is symbolic ai and other is connection is ai so whatever we are going to study in this course about neural networks and all that probably falls in connection is ai paradigm and there was no interest in this and people i mean it is hard to get funding and so on for these seventeen to eighteen years and that was largely triggered by this study that was done by minsky and papad and interestingly they were also often miscoated on what they had actually said in their paper so they had said a single perceptron cannot do it they in fact said that a multi layer network of perceptrons can do it but no one focused on the second part that a multi layer network of perceptron people started pushing the idea that a perceptron cannot do it and hence we should not be investigating it and so on so that is what happened for a long time as is known as the winter or the first winter then around one thousand nine hundred and eightysix actually came this algorithm which is known as back propagation again this is an algorithm that we are going to cover in a lot of detail in the course in the fourth or fifth lecture and this algorithm actually enables to train a deep neural network so deep network of neurons is something that you can train using this algorithm now this algorithm was actually popularized by rummel at rummelhart and others in one thousand nine hundred and eightysix but it is not completely discovered by them this was also around in various other fields so it was there in i think in systems analysis or something like that it was being used for other purposes in a different context and so on and rummelhart and others in one thousand nine hundred and eightysix were the first to kind of popularize it in the context of deep neural networks and this was a very important discovery because even today all the neural networks or most of them are trained using back propagation and of course there have been several other advances but the core remains the same that you use back propagation to train a deep neural network so something this was discovered almost thirty years back is still primarily used for training deep neural networks that is why this was a very important paper or breakthrough at that time and around the same time so interestingly so back propagation is used in conjunction with something known as gradient descent which was again discovered way back in one thousand eight hundred and fortyseven by cauchy and he was interested in using this to compute the orbit of heavenly bodies so that is something that people care about at that time today of course we use it for various other purposes one of them being discovering cats in videos or even for medical imaging or for describing whether a certain type of cancer is being depicted in a xray or things like that there is a lot of other purposes for which deep neural networks enhance and hence back propagation gradient descent and other things are being used for it but again these are not very modern discoveries these are dated way back thirty years and even gradient descent is almost one hundred and fifty years and so on so that is what i wanted to emphasize and around the same time in one thousand nine hundred and ninety or one thousand nine hundred and eightynine there is this another interesting theorem which is proved which is known as the universal approximation theorem and this is again something that we will cover in the course the third lecture or something like that where we will talk about the power of a deep neural network so again the importance of this or why this theorem was important will become clear later on when we cover it in detail but for now it is important to understand that what this theorem said is that if you have a deep neural network you could basically model all types of functions continuous functions to any desired precision so what it means in very layman terms is that if the way you make decisions using a bunch of inputs is a very very complex function of the input then you can have a neural network which will be able to learn this function right in many layman terms that is what it means and if i have to hype it up a bit or i have to say it in a very enthuse and excited manner i would say that basically it says that deep neural networks can be used for solving all kinds of machine learning problems right that is roughly what it says but with a pinch of salt and lot of caveats but that is what it means at least in the context of this course right so this is all around one thousand nine hundred and eightynine and despite this happening it some important discoveries towards the late end of eightys which was back propagation universal approximation theorem people were still not being able to use deep neural networks for really solving large practical problems right and a few challenges there was of course the compute power at that time was not at a level where it could support deep neural networks we did not have enough data for training deep neural networks and also in terms of techniques while back propagation is a sound technique it is to fail when you have really deep neural networks so when people tried training a very deep neural network they found that the training does not really converge the system does not really learn anything and so on and there were certain issues with using back propagation of the shelf at that time because of which it was not very successful right so again despite these slight boom around eightysix to ninety where some important discoveries were made and even follow up in ninetytwo ninetythree and so on there is still not a real big hype around deep neural networks or artificial neural networks at that time and there was again a slump a slow winter right up till two thousand and six