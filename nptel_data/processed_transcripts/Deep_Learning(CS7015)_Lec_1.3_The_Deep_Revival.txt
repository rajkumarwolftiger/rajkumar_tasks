when this deep revival happened right so in two thousand and six a very important study was or a very importantcontribution was made by hinton and salakut dinauf sorry if i have not pronounced it properly and they found that a method for training very very deep neural networks effectively now again the details of this are not important we will be doing that in the course at some point but what is the important take of a here is that while from one thousand nine hundred and eightynine to two thousand and six we knew that the data was not deep neural networks and they can potentially be used for solving a wide range of problems so because that is what the universal approximation theorem said but the problem was that in practice we are not being able to use it for much right it was not easy to train these networks but now with this technique there was revived interest and hope that now actually can train very very deep neural networks for a lot of practical problems so that is sparked of the interest again and then people started looking at all sorts of things and even this particular study which was done in two thousand and six was actually very similar to something done way back in one thousand nine hundred and ninetyoneninetythree and which again showed that you can train a very very deep neural network but again due to several factors maybe at that time due to the computational requirements or the data requirements or whatever i am not too sure about that it did not become so popular then but by two thousand and six probably the stage was much better for these kind of networks or techniques to succeed so then it became popular in two thousand and six and then this two thousand and six to two thousand and nine people started gaining more and more insights into the effectiveness of this discovery made by hinton and others which was this unsupervised pretraining right that is what i spoke about on the previous slide unsupervised pretraining and they started getting more and more insights into how you can make deep neural networks really work right so they came up with various techniques some of which we are going to study in this course so this was about how do you initialize the network better what is the better optimization algorithm to use what is the better regularization algorithm to use and so on so there were many things which were which started coming out in this period two thousand and six to two thousand and nine and by two thousand and nine everyone started taking note of this and again deep neural networks of artificial neural network started becoming popular that is when people realized that all this all the negative things that were tied to it that you are not able to train at well and so on have slowly people have started finding solutions to get by those and maybe you should start again focusing on the potential of deep neural networks and see if they can be used for a large scale practical application right so this two thousand and six to two thousand and nine was again a slow boom period where people were again trying to do a lot of work to popularize deep neural networks and get rid of some of the problems which existed in training and now from two thousand and nine onwards there was this a series of successes which kind of caught everyone which made everyone to stand up and take notice right that this is really working for a lot of practical applications starting with handwriting recognition so around two thousand and nine these guys one handwriting recognition competition in arabic and they did way better than the computer systems using a deep neural network and then there was success so this was in handwriting recognition then there was speech so it was shown that various existing systems the error rate of these systems could be seriously significantly reduced by using deep neural networks or plugging in a deep neural network component to existing systems right so this was handwriting then speech then again some kind of recognition which was on hand right hand rigid recognition for mnist this was a very popular data set which had been around since ninetyeight and a new record was set on this data set right this is the highest accuracy that was achieved on this data set around that time in two thousand and ten sorry and this is also the time when gpus enter the scene right so before that all of the stuff was being done on cpus but then people realized that very deep neural networks require a lot of computation and a lot of this computation can happen very quickly on gpus as opposed to cpus so people started using gpus for training and that drastically reduced the training and inference time so that was again something which sparked a lot of interest right because even though these were successful they were taking a lot of time to train but now with gpus you could even take care of that and this success continued so people started gaining or getting success in other fields like visual pattern recognition so this was a competition on recognizing traffic signboards and here again a deep neural network did way better than its other competitors and then the most popular or the one thing which made neural networks really popular was this imagenet challenge which was around since two thousand and eight or two thousand and nine and before two thousand and twelve when this lxnet was one of the participating systems in this competition most of the systems are non neural network based systems and this competition is basically about classifying a given image into one of one thousand classes right so this could be an image of a bird or a dog or a human or car truck and so on so you have to identify the right class of the main object in the image right so and in two thousand and twelve this lxnet which was a deep neural network or a convolutional neural network based system was able to actually outperform all the other systems by margin of sixtyseven percent right so the error for this system was sixteen percent and this was a deep neural network because it had eight layers the next year this was improved further and something known as zf net was proposed which was again eight layers but it did better than lxnet the next year even a deeper network with nineteen layers was proposed which did significantly better than lxnet then google entered the scene and they proposed something which is twentytwo layers and again reduced the error then microsoft joined in and they proposed something which had one hundred and fiftytwo layers and the error that you see here is actually better than what humans do right so even if a human was asked to label this image because of certain law certain noise in the image and so on even a human is bound to make more errors than threesix percent right that means even if you show one hundred images to humans he or she is bound to make go wrong on more than three or four of these images right whereas this system was able to get an error of threesix percent over the large test set so this two thousand and twelve to sixteen period where there was this continuous success on the image net challenge as well as success in other fields like natural language processing handwriting recognition speech and so on so this is the period where now everyone started talking about deep learning and a lot of companies started investing in right a lot of the traditional systems which were not deep neural network base were now started the people started converting them to deep neural network base system right so translation systems speed systems image classification object detection and so on there were lot of success in all these fields using deep neural networks and this particular thing that we are talking about which is image net and the success in this was driven by something known as convolutional neural networks