so now so this is what the progression was right that in two thousand and six people started or the study by hinton and others led to this revival and then people started realizing that deep neural networks and actually be useful a lot of practical applications and actually beat a lot of existing systems but there are still some problems and we still need to make these system more robust faster and even scale higher accuracies and so on right so imperilently while there was a lot of success happening from two thousand and twelve to two thousand and sixteen or even two thousand and ten to two thousand and sixteen in parallel there was also a lot of research to find better optimization algorithms which could lead to better conversions better accuracies and again some of the older ideas which were proposed way back in one thousand nine hundred and eightythree now this is again something that we will do in the course so most of the things that i am talking about we are going to cover in the course so we are going to talk about the image net challenge we are going to talk about all those networks the winning networks that i listed there alex net zfnet google net and so on so we are going to talk about the image net challenge we are going to talk about all those networks the winning networks that i listed there alex net zfnet google net and so on so we are going to talk about the image net challenge we are going to talk about all those networks the winning networks that i listed there alex net zfnet google net and so on so we are going to talk about nestor of gradient descent which is listed on this slide and many other better optimization methods which were proposed starting from two thousand and eleven so there was this parallel resource happening while people are getting a lot of success using traditional neural networks there is also interest in making them better and robust and lead to faster conversions and better accuracies and so on so this led to a lot of interest in coming up with better optimization algorithms and there was a series of these proposed starting from two thousand and eleven so there was a series of these proposed starting from two thousand and eleven so there was a series of these proposed starting from two thousand and eleven so adagrad is again something that we will do in the course rms prop adam eve and many more so many new algorithms have been proposed and in parallel a lot of other regularization techniques or weight initialization strategies have also been proposed for example batch normalization or xavier initialization and so on so these are all things which were aimed at making neural networks perform even better or faster and even reach better solutions or better accuracies and so on this is all that we are going to see in the course at some point or the other