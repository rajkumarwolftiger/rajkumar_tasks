{"audio_filepath": "processed_audio/Deep_Learning(CS7015)_Lec_1.1_Biological_Neuron.wav", "duration": 375.72, "text": "hello everyone welcome to lecture one of cs seven thousand and fifteen which is the course on deep learning in todays lecture is going to be a bit nontechnical we are not going to cover any technical concepts we are only going to talk about a brief or partial history of deep learning so we hear the terms artificial neural networks artificial neurons quite often these days and i just wanted you to take you through the journey of where does all this originate from and this history contains several spans across several fields not just computer science we will start with biology then talk about something in physics then eventually come to computer science and so on so with that let us start so just some acknowledgments and disclaimers so i have taken a lot of this material from the first paper which i have mentioned on the bullet and there might still be some errors because it is dates as black as one thousand eight hundred and seventyone so maybe i have got some of the facts wrong so feel free to contact me if you think some of these portions need to be corrected and it would be good if you could provide me appropriate references for these corrections so let us start with the first chapter which is on biological neurons as i said it spans several fields so we will start with biology and we will first talk about the brain and the neurons within the brain so way back in one thousand eight hundred and seventyone one thousand eight hundred and seventythree around that time joseph von gelach actually proposed that the nervous system our nervous system is a single continuous network as opposed to a network of many discrete cells so his idea was that this is one gigantic cell setting in our nervous system and it is not a network of discrete cells and this theory was known as the reticular theory and around the same time there was this some breakthrough or some progress in staining techniques where camilo golgi discovered that a chemical reaction that will allow you to examine the neurons or neurons or the nervous tissue so he was looking at this nervous tissue using some staining technique and by looking at what you see in this figure on the right hand side the yellow figure even he concluded that this is just one single cell and not a network of discrete cells so he was again a proponent of reticular theory so this is about camilo golgi and then interestingly santiago cial he was the same technique which golgi proposed and he studied the same tissue and he came up with the conclusion that this is not a single cell this is actually collection of various discrete cells which together forms a network so it is a network of things as opposed to a single cell there so that is what his theory was and this was eventually came to be known as the neuron doctrine although this was not consolidated in the form of a doctrine by cial that was done by this gentleman so he coined the term neurons so now today when you think about here about artificial neural networks or artificial neurons the term neuron actually originated way back in one thousand eight hundred and ninetyone and this gentleman was responsible for coining that and he was also responsible for consolidating the neuron doctrine so already as you saw in the previous slide cial had proposed it but then over the years many people bought this idea and this guy was responsible for consolidating that into a neuron doctrine interestingly he is not only responsible for coining the term neuron he is also responsible for coining the term chromosome so two very important terms were coined by this one person so now here is a question so around one thousand nine hundred and six when it was time to give the noble price in medicine what do you think which of these two proponents there are two theories one is reticular theory which is a single cell and then there is this neuron doctrine which is a collection of cells or collection of neurons that the nervous system is a collection of neurons so what do you think which of these two guys who are proponents of these two different theories who would have got the actual noble price for medicine so interestingly it is given to both of them so till one thousand nine hundred and six in fact way later till one thousand nine hundred and fifty also this debate was not completely settled and then the committee said both of these are interesting pieces of work we yet do not know what really actually what the situation is actually but these conflictic ideas have a place together and so the noble price was actually given to both of them and this led to a history of some kind of controversies between these two scientists and so on and this debate surprisingly was settled way later in one thousand nine hundred and fifty and not by progress in biology as such but by progress in a different field so this was with the advent of electron microscopy so now it was able to see this at a much better scale and by looking at this under a microscope it was found that actually there is a gap between these neurons and hence it is not one single cell it is actually a collection or a network of cells with a clear gap between them or some connections between them which are now known as synapses so this was when the debate was settled so now why am i talking about biology why am i telling you about biological neuron and so on so this is what we need to understand so there has always been interesting understanding how the human brain works from a biological perspective at least and around this time the debate was more or less settled that we have this our brain is a collection of many neurons and they interact with each other to help us do a lot of complex processing that we do on a daily basis right from getting up in the morning and deciding what do we want to do today taking decisions performing computations and various complex things that our brain is capable of doing right now the interest is in seeing if you understand how the brain works can we make an artificial model for that right so can we come up with something which can simulate how our brain works and what is that model and how do we make a computer do that or how do we make a machine do that right so that is why i started from biological neurons to take the inspiration from biology"}
{"audio_filepath": "processed_audio/Deep_Learning(CS7015)_Lec_1.2_From_Spring_to_Winter_of_AI.wav", "duration": 781.272, "text": "and now what we will get into in the next chapter is we will start talking about artificial intelligence and this is titled as from the spring to the winter of ai so i am going to talk about when was this boom in ai started or when is that people started thinking and talking about ai seriously and what eventually happened to the initial boom and so on right so let us start with one thousand nine hundred and fortythree whereas as i was saying that there was a lot of interest in understanding how does the human brain work and then come up with a computer computational or mathematical model of that right so mcculloch and pitts one of them was a neuroscientist and the other one was a logician right so no computer scientist or anything at that point of time and they came up with this extremely simplified model that just as a brain takes the input from lot of factors right so now suppose you want to decide whether you want to go out for a movie or not so you would probably think about do you really have any exams coming up that could be your factor xone you could think about is the weather good to go out is it training would it be difficult to go out at this point would there be a lot of traffic is it a very popular movie and hence tickets may not be available and so on right so a brain kind of presses all this information you might also look at things like the reviews of the movie or the imdb rating of the movie and so on and based on all these complex inputs it applies some function and then takes a decision yes or no that ok i want to probably go for a movie so this is an overly simplified model of how the brain works is and what this model says is that you take inputs from various sources and based on that you come up with a binary decision right so this is what they proposed in one thousand nine hundred and fortythree so now we have come to an artificial neuron so this is not a biological neuron this is how you would implement it as a machine right so that was in one thousand nine hundred and fortythree then along and then this kind of led to a lot of boom in our interest in artificial intelligence and so on and i guess around one thousand nine hundred and fiftysix in a conference the term artificial intelligence was formally coined and within a one or two years from there frank rosenwald came up with this perceptron model of doing computations or perceptron model of what an artificial neuron could be and we will talk about this in detail later on the course and not tell you what these things are as of now just think of the a new model was proposed and this is what he had to say about this model right so he said that the perceptron may eventually be able to learn make decisions and translate languages we find anything odd about this statement yes so learn and make decisions make sense but why translate languages why is so specific why is such a specific interest in languages right so that you have to connect back to history right so this is also the period of the cold war and there was always always a lot of interest a lot of research in translation was actually fueled by the world war and even said happened after that where these countries which were at loggerheads with each other they wanted to understand what the other countries doing but they did not speak each others language that is why there was a lot of interest from sp on edge point of view or from spying and so on to be able to translate languages and hence that specific required and a lot of this research would have been funded from agencies which are interested in these things right and the defense or war or something ok so and this work was largely done for the navy and this is an this is an extract from the article written in new york times way back in one thousand nine hundred and fiftyseven or fiftyeight where it was mentioned that the embryo of an this perceptron is an embryo of an electronic computer that the navy expects will be able to walk talk see write reproduce itself and be conscious of its existence so i am not quoting something from two thousand and seventeen or eighteen this is way back in one thousand nine hundred and fiftyseven and fiftyeight right and that is why i like the history part of it so recently there is a lot of boom or lot of hype around ai that ai will take over a lot of things it will take over jobs it might eventually we might be colonized by ai agents and so on so i just want to emphasize that i do not know whether that will happen or not but this is not something new we have been talking about the promise of ai as far back since one thousand nine hundred and fiftyseven one thousand nine hundred and fiftyeight right this is not something new that people are talking about now it is always been there and to what extent this promise will be fulfilled is yet to be seen and of course as compared to one thousand nine hundred and fiftyseven fiftyeight we have made a lot of progress in other fields which have enabled ai to be much more successful than it was earlier for example we have much better compute power now we have lots of data now things to the internet and other things that you can actually crawl tons and tons of data and then try to learn something from a data or try to make the machine learn something from data so we have made a lot of progress in other aspects where which ai is now at a position where it can really make a difference but this wanted to say that these are not things which have not been said in the past it has always been that ai has always been considered to be very promising and perhaps a bit hyped also right so that is about one thousand nine hundred and fiftyseven fiftyeight then now what we talk about is for the past eight to ten years at least when we talk about ai we are talking about deep learning and that is what this course is about largely about deep learning i am not saying that other and what deep learning is largely about if i want to tell you in a very layman nutshell term is it is about a large number of artificial neurons connected to each other in layers and functioning towards achieving certain goal so this is like a schematic of what a deep neural network or a feed forward neural network would look like so this is again not something new which has come up in the last eight to ten years although people have started discussing it a lot in the last eight to ten years look at it way back in one thousand nine hundred and sixtyfive sixtyeight suppose something which looked very much like a modern deep neural network or a modern feed forward neural network and in many circles he is considered to be one of the founding fathers of modern deep learning right so that is about that sixtyeight right from one thousand nine hundred and fortythree to one thousand nine hundred and sixtyeight it was mainly about the spring time for ai and what i mean by that that every one was showing interest in that the government was funding a lot of research in ai and people really thought that ai could deliver a lot of things on a lot of fronts for various applications healthcare defense and so on and then around one thousand nine hundred and sixtynine an interesting paper came out by these two gentlemen minsky and papard which essentially outlined some limitations of the perceptron model right and we will talk about these limitations later on in the course in the second or third lecture but for now i will not get into details of that but what it said that it is possible that a perceptron cannot handle some very simple functions also so you are trying to make the perceptron learn some very complex functions because the way we decide how to watch a movie is a very complex function of the inputs that we considered but even a simple function like xr is something which a perceptron cannot be used to model that is what this paper essentially showed and this led to severe criticism for ai and then people started losing interest in ai and lot of government funding actually subsided after one thousand nine hundred and sixtynine all the way to one thousand nine hundred and eightysix actually this was the ai winter of connectionism so there was very little interest in connection is ai so there are two types of ai one is symbolic ai and other is connection is ai so whatever we are going to study in this course about neural networks and all that probably falls in connection is ai paradigm and there was no interest in this and people i mean it is hard to get funding and so on for these seventeen to eighteen years and that was largely triggered by this study that was done by minsky and papad and interestingly they were also often miscoated on what they had actually said in their paper so they had said a single perceptron cannot do it they in fact said that a multi layer network of perceptrons can do it but no one focused on the second part that a multi layer network of perceptron people started pushing the idea that a perceptron cannot do it and hence we should not be investigating it and so on so that is what happened for a long time as is known as the winter or the first winter then around one thousand nine hundred and eightysix actually came this algorithm which is known as back propagation again this is an algorithm that we are going to cover in a lot of detail in the course in the fourth or fifth lecture and this algorithm actually enables to train a deep neural network so deep network of neurons is something that you can train using this algorithm now this algorithm was actually popularized by rummel at rummelhart and others in one thousand nine hundred and eightysix but it is not completely discovered by them this was also around in various other fields so it was there in i think in systems analysis or something like that it was being used for other purposes in a different context and so on and rummelhart and others in one thousand nine hundred and eightysix were the first to kind of popularize it in the context of deep neural networks and this was a very important discovery because even today all the neural networks or most of them are trained using back propagation and of course there have been several other advances but the core remains the same that you use back propagation to train a deep neural network so something this was discovered almost thirty years back is still primarily used for training deep neural networks that is why this was a very important paper or breakthrough at that time and around the same time so interestingly so back propagation is used in conjunction with something known as gradient descent which was again discovered way back in one thousand eight hundred and fortyseven by cauchy and he was interested in using this to compute the orbit of heavenly bodies so that is something that people care about at that time today of course we use it for various other purposes one of them being discovering cats in videos or even for medical imaging or for describing whether a certain type of cancer is being depicted in a xray or things like that there is a lot of other purposes for which deep neural networks enhance and hence back propagation gradient descent and other things are being used for it but again these are not very modern discoveries these are dated way back thirty years and even gradient descent is almost one hundred and fifty years and so on so that is what i wanted to emphasize and around the same time in one thousand nine hundred and ninety or one thousand nine hundred and eightynine there is this another interesting theorem which is proved which is known as the universal approximation theorem and this is again something that we will cover in the course the third lecture or something like that where we will talk about the power of a deep neural network so again the importance of this or why this theorem was important will become clear later on when we cover it in detail but for now it is important to understand that what this theorem said is that if you have a deep neural network you could basically model all types of functions continuous functions to any desired precision so what it means in very layman terms is that if the way you make decisions using a bunch of inputs is a very very complex function of the input then you can have a neural network which will be able to learn this function right in many layman terms that is what it means and if i have to hype it up a bit or i have to say it in a very enthuse and excited manner i would say that basically it says that deep neural networks can be used for solving all kinds of machine learning problems right that is roughly what it says but with a pinch of salt and lot of caveats but that is what it means at least in the context of this course right so this is all around one thousand nine hundred and eightynine and despite this happening it some important discoveries towards the late end of eightys which was back propagation universal approximation theorem people were still not being able to use deep neural networks for really solving large practical problems right and a few challenges there was of course the compute power at that time was not at a level where it could support deep neural networks we did not have enough data for training deep neural networks and also in terms of techniques while back propagation is a sound technique it is to fail when you have really deep neural networks so when people tried training a very deep neural network they found that the training does not really converge the system does not really learn anything and so on and there were certain issues with using back propagation of the shelf at that time because of which it was not very successful right so again despite these slight boom around eightysix to ninety where some important discoveries were made and even follow up in ninetytwo ninetythree and so on there is still not a real big hype around deep neural networks or artificial neural networks at that time and there was again a slump a slow winter right up till two thousand and six"}
{"audio_filepath": "processed_audio/Deep_Learning(CS7015)_Lec_1.3_The_Deep_Revival.wav", "duration": 435.048, "text": "when this deep revival happened right so in two thousand and six a very important study was or a very importantcontribution was made by hinton and salakut dinauf sorry if i have not pronounced it properly and they found that a method for training very very deep neural networks effectively now again the details of this are not important we will be doing that in the course at some point but what is the important take of a here is that while from one thousand nine hundred and eightynine to two thousand and six we knew that the data was not deep neural networks and they can potentially be used for solving a wide range of problems so because that is what the universal approximation theorem said but the problem was that in practice we are not being able to use it for much right it was not easy to train these networks but now with this technique there was revived interest and hope that now actually can train very very deep neural networks for a lot of practical problems so that is sparked of the interest again and then people started looking at all sorts of things and even this particular study which was done in two thousand and six was actually very similar to something done way back in one thousand nine hundred and ninetyoneninetythree and which again showed that you can train a very very deep neural network but again due to several factors maybe at that time due to the computational requirements or the data requirements or whatever i am not too sure about that it did not become so popular then but by two thousand and six probably the stage was much better for these kind of networks or techniques to succeed so then it became popular in two thousand and six and then this two thousand and six to two thousand and nine people started gaining more and more insights into the effectiveness of this discovery made by hinton and others which was this unsupervised pretraining right that is what i spoke about on the previous slide unsupervised pretraining and they started getting more and more insights into how you can make deep neural networks really work right so they came up with various techniques some of which we are going to study in this course so this was about how do you initialize the network better what is the better optimization algorithm to use what is the better regularization algorithm to use and so on so there were many things which were which started coming out in this period two thousand and six to two thousand and nine and by two thousand and nine everyone started taking note of this and again deep neural networks of artificial neural network started becoming popular that is when people realized that all this all the negative things that were tied to it that you are not able to train at well and so on have slowly people have started finding solutions to get by those and maybe you should start again focusing on the potential of deep neural networks and see if they can be used for a large scale practical application right so this two thousand and six to two thousand and nine was again a slow boom period where people were again trying to do a lot of work to popularize deep neural networks and get rid of some of the problems which existed in training and now from two thousand and nine onwards there was this a series of successes which kind of caught everyone which made everyone to stand up and take notice right that this is really working for a lot of practical applications starting with handwriting recognition so around two thousand and nine these guys one handwriting recognition competition in arabic and they did way better than the computer systems using a deep neural network and then there was success so this was in handwriting recognition then there was speech so it was shown that various existing systems the error rate of these systems could be seriously significantly reduced by using deep neural networks or plugging in a deep neural network component to existing systems right so this was handwriting then speech then again some kind of recognition which was on hand right hand rigid recognition for mnist this was a very popular data set which had been around since ninetyeight and a new record was set on this data set right this is the highest accuracy that was achieved on this data set around that time in two thousand and ten sorry and this is also the time when gpus enter the scene right so before that all of the stuff was being done on cpus but then people realized that very deep neural networks require a lot of computation and a lot of this computation can happen very quickly on gpus as opposed to cpus so people started using gpus for training and that drastically reduced the training and inference time so that was again something which sparked a lot of interest right because even though these were successful they were taking a lot of time to train but now with gpus you could even take care of that and this success continued so people started gaining or getting success in other fields like visual pattern recognition so this was a competition on recognizing traffic signboards and here again a deep neural network did way better than its other competitors and then the most popular or the one thing which made neural networks really popular was this imagenet challenge which was around since two thousand and eight or two thousand and nine and before two thousand and twelve when this lxnet was one of the participating systems in this competition most of the systems are non neural network based systems and this competition is basically about classifying a given image into one of one thousand classes right so this could be an image of a bird or a dog or a human or car truck and so on so you have to identify the right class of the main object in the image right so and in two thousand and twelve this lxnet which was a deep neural network or a convolutional neural network based system was able to actually outperform all the other systems by margin of sixtyseven percent right so the error for this system was sixteen percent and this was a deep neural network because it had eight layers the next year this was improved further and something known as zf net was proposed which was again eight layers but it did better than lxnet the next year even a deeper network with nineteen layers was proposed which did significantly better than lxnet then google entered the scene and they proposed something which is twentytwo layers and again reduced the error then microsoft joined in and they proposed something which had one hundred and fiftytwo layers and the error that you see here is actually better than what humans do right so even if a human was asked to label this image because of certain law certain noise in the image and so on even a human is bound to make more errors than threesix percent right that means even if you show one hundred images to humans he or she is bound to make go wrong on more than three or four of these images right whereas this system was able to get an error of threesix percent over the large test set so this two thousand and twelve to sixteen period where there was this continuous success on the image net challenge as well as success in other fields like natural language processing handwriting recognition speech and so on so this is the period where now everyone started talking about deep learning and a lot of companies started investing in right a lot of the traditional systems which were not deep neural network base were now started the people started converting them to deep neural network base system right so translation systems speed systems image classification object detection and so on there were lot of success in all these fields using deep neural networks and this particular thing that we are talking about which is image net and the success in this was driven by something known as convolutional neural networks"}
{"audio_filepath": "processed_audio/Deep_Learning(CS7015)_Lec_1.4_From_Cats_to_Convolutional_Neural_Networks.wav", "duration": 169.824, "text": "i will talk about the history of convolutional neural networks and i call this part of history as cats and it will become obvious why i call it so so around one thousand nine hundred and fiftynine hubel and wiesel did this famous experiment there is still i think you could see some videos of it on youtube where there is a cat and there is a screen in front of it and on screen there were these lines being displayed at different locations and at different locations and at different locations so slanted horizontal vertical and so on and there are some electrodes fitted to the cat and they were measuring trying to measure that which parts of brain actually respond to different visual stimuli so if you show it stimulus at a certain location there is a different part of the brain fire and so on so and one of the things of outcomes of the study was that different neurons in brain fire to only different types of stimuli it is not that all neurons in brain are always fired to any kind of visual stimuli that you give to them so this is essentially roughly the idea behind convolutional neural networks starting from something known as neocognitron which was proposed way back in one thousand nine hundred and eighty you could think of it as a very primitive convolutional neural network i am sure that most of you have now read about or heard about convolutional neural networks but something very similar to it was proposed way back in one thousand nine hundred and eighty and what we know as the modern convolutional neural networks may be i think jan likun is someone who proposed them way back in one thousand nine hundred and eightynine and he was interested in using them for the task of handwritten digit recognition and this was again in the context of postal delivery services so a lot of pin codes get written or phone numbers get written on the postcards and there is a requirement to read them automatically so that they can be the letters or postcards can be separated into different categories according to the postcard according to the postal code and so on so or the pin code so that is where this interest was there and one thousand nine hundred and eightynine was when this convolutional neural networks were first proposed or used for this task and then over the years several improvements were done to that and in one thousand nine hundred and ninetyeight this now famous dataset the mnist dataset which is used for teaching deep neural networks courses or even for initial experiments with various neural network based networks this is one of the popular datasets which is used in this field and this was again released way back in one thousand nine hundred and ninetyeight and even today even for my course i use it for various assignments and so on so it is interesting that an algorithm which was inspired by an experiment on cats is today used to detect cats in videos of course among other various other things is just i am just jokingly saying this"}
{"audio_filepath": "processed_audio/Deep_Learning(CS7015)_Lec_1.5_Faster,_higher,_stronger.wav", "duration": 125.544, "text": "so now so this is what the progression was right that in two thousand and six people started or the study by hinton and others led to this revival and then people started realizing that deep neural networks and actually be useful a lot of practical applications and actually beat a lot of existing systems but there are still some problems and we still need to make these system more robust faster and even scale higher accuracies and so on right so imperilently while there was a lot of success happening from two thousand and twelve to two thousand and sixteen or even two thousand and ten to two thousand and sixteen in parallel there was also a lot of research to find better optimization algorithms which could lead to better conversions better accuracies and again some of the older ideas which were proposed way back in one thousand nine hundred and eightythree now this is again something that we will do in the course so most of the things that i am talking about we are going to cover in the course so we are going to talk about the image net challenge we are going to talk about all those networks the winning networks that i listed there alex net zfnet google net and so on so we are going to talk about the image net challenge we are going to talk about all those networks the winning networks that i listed there alex net zfnet google net and so on so we are going to talk about the image net challenge we are going to talk about all those networks the winning networks that i listed there alex net zfnet google net and so on so we are going to talk about nestor of gradient descent which is listed on this slide and many other better optimization methods which were proposed starting from two thousand and eleven so there was this parallel resource happening while people are getting a lot of success using traditional neural networks there is also interest in making them better and robust and lead to faster conversions and better accuracies and so on so this led to a lot of interest in coming up with better optimization algorithms and there was a series of these proposed starting from two thousand and eleven so there was a series of these proposed starting from two thousand and eleven so there was a series of these proposed starting from two thousand and eleven so adagrad is again something that we will do in the course rms prop adam eve and many more so many new algorithms have been proposed and in parallel a lot of other regularization techniques or weight initialization strategies have also been proposed for example batch normalization or xavier initialization and so on so these are all things which were aimed at making neural networks perform even better or faster and even reach better solutions or better accuracies and so on this is all that we are going to see in the course at some point or the other"}
